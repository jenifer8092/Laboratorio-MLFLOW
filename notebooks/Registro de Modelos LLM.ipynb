{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üßë‚Äçüíª Introducci√≥n a MLFLow (Parte II): Tracking de Modelos de Lenguaje (LLMs).\n",
    "Integrantes: Tob√≠as Romero **(2021214011)** y Jenifer Roa **(2022214006)**\n",
    "---"
   ],
   "id": "hdr"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Importaci√≥n de librer√≠as.",
   "id": "sec1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "import google.generativeai as genai\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv"
   ],
   "id": "imports",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Configuraci√≥n de API Keys y DagsHub/MLflow.",
   "id": "sec2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cargar .env\n",
    "load_dotenv()\n",
    "\n",
    "# --- API Keys ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "openrouter_client = OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=OPENROUTER_API_KEY)\n",
    "\n",
    "# --- DagsHub + MLflow remoto ---\n",
    "import dagshub\n",
    "dagshub.init(repo_owner='jenifer8092', repo_name='Laboratorio-MLFLOW', mlflow=True)\n",
    "\n",
    "# (Opcional) Nombre del experimento por ENV\n",
    "experiment_name = os.getenv(\"EXPERIMENT_NAME\", \"LLM_Comparison_Gemini_vs_Deepseek\")\n",
    "\n",
    "descripcion = \"\"\"\n",
    "Comparaci√≥n de modelos LLM (Gemini vs DeepSeek).\n",
    "Incluye: prompts controlados, datasets de prueba y m√©tricas autom√°ticas\n",
    "(latencia, tokens y coste estimado por token).\n",
    "\"\"\"\n",
    "tags_exp = {\n",
    "    \"owner\": \"Tob√≠as Romero\",\n",
    "    \"proyecto\": \"MLOps\",\n",
    "    \"model_family\": \"LLM\",\n",
    "    \"providers\": \"Gemini, DeepSeek(OpenRouter)\",\n",
    "    \"tracking\": \"notebook\",\n",
    "}\n",
    "\n",
    "client = MlflowClient()\n",
    "exp = mlflow.get_experiment_by_name(experiment_name)\n",
    "if exp and getattr(exp, \"lifecycle_stage\", None) == \"deleted\":\n",
    "    client.restore_experiment(exp.experiment_id)\n",
    "    exp = mlflow.get_experiment_by_name(experiment_name)\n",
    "if exp is None:\n",
    "    exp_id = client.create_experiment(experiment_name, tags=tags_exp)\n",
    "else:\n",
    "    exp_id = exp.experiment_id\n",
    "    for k, v in tags_exp.items():\n",
    "        client.set_experiment_tag(exp_id, k, v)\n",
    "client.set_experiment_tag(exp_id, \"mlflow.note.content\", descripcion)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"‚úì Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "exp_actualizado = mlflow.get_experiment(exp_id)\n",
    "print(f\"‚úì Experimento: {exp_actualizado.name} | ID: {exp_actualizado.experiment_id}\")\n",
    "print(f\"‚úì Artifacts: {exp_actualizado.artifact_location}\")\n",
    "print(f\"‚úì Tags: {exp_actualizado.tags}\")"
   ],
   "id": "mlflow_cfg",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Definici√≥n de tareas y prompts.",
   "id": "sec3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "TASKS = {\n",
    "    \"creative_writing\": {\n",
    "        \"prompt\": \"Escribe un cuento corto de ciencia ficci√≥n sobre un robot que aprende a sentir emociones. M√°ximo 200 palabras.\",\n",
    "        \"description\": \"Tarea de escritura creativa y narrativa\"\n",
    "    },\n",
    "    \"code_generation\": {\n",
    "        \"prompt\": \"Genera una funci√≥n en Python que implemente el algoritmo de b√∫squeda binaria con comentarios explicativos.\",\n",
    "        \"description\": \"Generaci√≥n de c√≥digo con documentaci√≥n\"\n",
    "    },\n",
    "    \"question_answering\": {\n",
    "        \"prompt\": \"Explica qu√© es el aprendizaje por refuerzo en machine learning y proporciona un ejemplo pr√°ctico de su aplicaci√≥n.\",\n",
    "        \"description\": \"Respuesta a preguntas t√©cnicas\"\n",
    "    },\n",
    "    \"summarization\": {\n",
    "        \"prompt\": \"Resume los principios fundamentales de la programaci√≥n orientada a objetos en 5 puntos clave.\",\n",
    "        \"description\": \"Resumen y s√≠ntesis de informaci√≥n\"\n",
    "    },\n",
    "    \"translation\": {\n",
    "        \"prompt\": \"Traduce el siguiente texto al ingl√©s de manera natural: 'El machine learning ha revolucionado la forma en que procesamos y analizamos grandes vol√∫menes de datos en tiempo real.'\",\n",
    "        \"description\": \"Traducci√≥n de texto t√©cnico\"\n",
    "    }\n",
    "}\n",
    "print(\"Tareas definidas:\")\n",
    "for task_name, task_info in TASKS.items():\n",
    "    print(f\"  ‚Ä¢ {task_name}: {task_info['description']}\")"
   ],
   "id": "tasks",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Helpers: costos, logging de artifacts.",
   "id": "sec4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Costos por 1K tokens (simulados o por ENV)\n",
    "COSTS = {\n",
    "    \"gemini\": {\n",
    "        \"in\": float(os.getenv(\"GEMINI_IN_COST_PER_1K\", \"0.00\")),\n",
    "        \"out\": float(os.getenv(\"GEMINI_OUT_COST_PER_1K\", \"0.00\")),\n",
    "    },\n",
    "    \"deepseek\": {\n",
    "        \"in\": float(os.getenv(\"DEEPSEEK_IN_COST_PER_1K\", \"0.00\")),\n",
    "        \"out\": float(os.getenv(\"DEEPSEEK_OUT_COST_PER_1K\", \"0.00\")),\n",
    "    }\n",
    "}\n",
    "\n",
    "def estimate_cost_usd(provider_key, input_tokens, output_tokens):\n",
    "    rates = COSTS.get(provider_key, {\"in\":0.0, \"out\":0.0})\n",
    "    return (input_tokens/1000.0)*rates[\"in\"] + (output_tokens/1000.0)*rates[\"out\"]\n",
    "\n",
    "def save_artifacts(task_name, prompt, response_data, model_name, temperature, artifact_root=\"llm_runs\"):\n",
    "    \"\"\"Sube artifacts directamente al artifact store del run.\"\"\"\n",
    "    assert mlflow.active_run() is not None, \"Debe haber un mlflow.start_run() activo.\"\n",
    "    base = f\"{artifact_root}/{task_name}\"\n",
    "    mlflow.log_text(f\"TAREA: {task_name}\\n{'='*80}\\n\\n{prompt}\\n\", artifact_file=f\"{base}/prompt.txt\")\n",
    "    mlflow.log_text(\n",
    "        f\"MODELO: {model_name}\\nTEMPERATURA: {temperature}\\nLATENCIA: {response_data['latency']:.3f}s\\n\"\n",
    "        f\"INPUT_TOKENS: {response_data['input_tokens']}\\nOUTPUT_TOKENS: {response_data['output_tokens']}\\n\"\n",
    "        + \"=\"*80 + f\"\\n\\n{response_data['response']}\\n\",\n",
    "        artifact_file=f\"{base}/response.txt\"\n",
    "    )\n",
    "    mlflow.log_dict({\n",
    "        \"model\": model_name,\n",
    "        \"task\": task_name,\n",
    "        \"temperature\": temperature,\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response_data['response'],\n",
    "        \"latency_seconds\": response_data['latency'],\n",
    "        \"input_tokens\": response_data['input_tokens'],\n",
    "        \"output_tokens\": response_data['output_tokens'],\n",
    "        \"total_tokens\": response_data['input_tokens'] + response_data['output_tokens'],\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "    }, artifact_file=f\"{base}/experiment.json\")\n"
   ],
   "id": "helpers",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Llamados a modelos LLMs (parametrizados).",
   "id": "sec5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def call_gemini(prompt, temperature=0.7, model_name=\"gemini-2.0-flash-exp\"):\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "    start_time = time.time()\n",
    "    response = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config=genai.types.GenerationConfig(temperature=temperature)\n",
    "    )\n",
    "    latency = time.time() - start_time\n",
    "    text = getattr(response, 'text', '')\n",
    "    if hasattr(response, 'usage_metadata') and response.usage_metadata:\n",
    "        input_tokens = int(response.usage_metadata.prompt_token_count or 0)\n",
    "        output_tokens = int(response.usage_metadata.candidates_token_count or 0)\n",
    "    else:\n",
    "        # Estimaci√≥n simple si el SDK no expone usage\n",
    "        input_tokens = int(len(prompt.split()) * 1.3)\n",
    "        output_tokens = int(len(text.split()) * 1.3)\n",
    "    return {\"response\": text, \"latency\": latency, \"input_tokens\": input_tokens, \"output_tokens\": output_tokens}\n",
    "\n",
    "def call_deepseek(prompt, temperature=0.7, model_name=\"deepseek/deepseek-chat\"):\n",
    "    start_time = time.time()\n",
    "    completion = openrouter_client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    latency = time.time() - start_time\n",
    "    response_text = completion.choices[0].message.content\n",
    "    usage = getattr(completion, 'usage', None)\n",
    "    input_tokens = int(getattr(usage, 'prompt_tokens', 0))\n",
    "    output_tokens = int(getattr(usage, 'completion_tokens', 0))\n",
    "    # Si falta usage, estimaci√≥n simple\n",
    "    if (input_tokens + output_tokens) == 0:\n",
    "        input_tokens = int(len(prompt.split()) * 1.3)\n",
    "        output_tokens = int(len(response_text.split()) * 1.3)\n",
    "    return {\"response\": response_text, \"latency\": latency, \"input_tokens\": input_tokens, \"output_tokens\": output_tokens}\n"
   ],
   "id": "calls",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. PythonModel stub (para registrar en el Model Registry).",
   "id": "sec6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class LLMStubModel(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, provider: str, model_name: str):\n",
    "        self.provider = provider\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        # Este stub solo documenta c√≥mo invocar el LLM real externamente.\n",
    "        # Devuelve un DF/Serie con un mensaje de placeholder por fila.\n",
    "        if hasattr(model_input, 'to_dict'):\n",
    "            n = len(model_input)\n",
    "        else:\n",
    "            try:\n",
    "                n = len(model_input)\n",
    "            except Exception:\n",
    "                n = 1\n",
    "        return [f\"Use provider={self.provider} model={self.model_name} para inferencia de LLM en producci√≥n.\"] * n\n"
   ],
   "id": "stub",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Runner de experimentos (con logging y registro en Registry).",
   "id": "sec7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_experiment(model_type, model_name, task_name, task_info, temperature=0.7):\n",
    "    run_name = f\"{model_type}_{task_name}\"\n",
    "    prompt = task_info[\"prompt\"]\n",
    "    provider = \"Google AI\" if model_type == \"gemini\" else \"OpenRouter\"\n",
    "    provider_key = \"gemini\" if model_type == \"gemini\" else \"deepseek\"\n",
    "\n",
    "    print(f\"\\n{'='*80}\\nEXPERIMENTO: {run_name}\\nTarea: {task_info['description']}\\n{'='*80}\\n\\n PROMPT:\\n{prompt}\\n\")\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        try:\n",
    "            if model_type == \"gemini\":\n",
    "                result = call_gemini(prompt, temperature, model_name)\n",
    "            else:\n",
    "                result = call_deepseek(prompt, temperature, model_name)\n",
    "            success = True\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\\n\")\n",
    "            result = {\"response\": f\"ERROR: {str(e)}\", \"latency\": 0, \"input_tokens\": 0, \"output_tokens\": 0}\n",
    "            success = False\n",
    "\n",
    "        print(\"=\"*80 + \"\\nRESPUESTA:\\n\" + \"=\"*80)\n",
    "        print(result['response'])\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "        # Params\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "        mlflow.log_param(\"model_type\", model_type)\n",
    "        mlflow.log_param(\"temperature\", temperature)\n",
    "        mlflow.log_param(\"task_type\", task_name)\n",
    "        mlflow.log_param(\"task_description\", task_info[\"description\"])\n",
    "        mlflow.log_param(\"provider\", provider)\n",
    "\n",
    "        # M√©tricas\n",
    "        total_tokens = result['input_tokens'] + result['output_tokens']\n",
    "        words_per_second = (len(result['response'].split()) / result['latency']) if result['latency'] > 0 else 0\n",
    "        est_cost = estimate_cost_usd(provider_key, result['input_tokens'], result['output_tokens'])\n",
    "\n",
    "        mlflow.log_metric(\"latency_seconds\", result['latency'])\n",
    "        mlflow.log_metric(\"input_tokens\", result['input_tokens'])\n",
    "        mlflow.log_metric(\"output_tokens\", result['output_tokens'])\n",
    "        mlflow.log_metric(\"total_tokens\", total_tokens)\n",
    "        mlflow.log_metric(\"response_length_chars\", len(result['response']))\n",
    "        mlflow.log_metric(\"words_per_second\", words_per_second)\n",
    "        mlflow.log_metric(\"estimated_cost_usd\", est_cost)\n",
    "        mlflow.log_metric(\"success\", 1 if success else 0)\n",
    "\n",
    "        # Artifacts\n",
    "        save_artifacts(task_name, prompt, result, model_name, temperature)\n",
    "\n",
    "        # Tags\n",
    "        mlflow.set_tag(\"model_family\", \"LLM\")\n",
    "        mlflow.set_tag(\"provider\", provider)\n",
    "        mlflow.set_tag(\"model_type\", model_type)\n",
    "        mlflow.set_tag(\"task_category\", task_name)\n",
    "        mlflow.set_tag(\"language\", \"espa√±ol\")\n",
    "        mlflow.set_tag(\"status\", \"success\" if success else \"failed\")\n",
    "        mlflow.set_tag(\"latency_tier\", \"fast\" if result['latency'] < 2 else (\"medium\" if result['latency'] < 5 else \"slow\"))\n",
    "\n",
    "        # Nota del run\n",
    "        mlflow.set_tag(\"mlflow.note.content\", f\"LLM {model_name} ({provider}) | Tarea: {task_info['description']} | Temp={temperature} | Lat={result['latency']:.3f}s | Tokens={total_tokens} | Cost‚âà${est_cost:.4f}\")\n",
    "\n",
    "        # Registrar un stub en el Model Registry (crea/actualiza versi√≥n)\n",
    "        registry_name = f\"llm_{model_type}_chat\"\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"model\",\n",
    "            python_model=LLMStubModel(provider=provider, model_name=model_name),\n",
    "            registered_model_name=registry_name,\n",
    "            pip_requirements=[\"pandas\"]\n",
    "        )\n",
    "\n",
    "        print(\"Experimento registrado en MLflow\")\n",
    "        print(f\"Registry: {registry_name}\\n\")\n"
   ],
   "id": "runner",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. Ejecutar suite de tareas para cada modelo.",
   "id": "sec8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "GEMINI_MODEL = os.getenv(\"GEMINI_MODEL\", \"gemini-2.0-flash-exp\")\n",
    "DEEPSEEK_MODEL = os.getenv(\"DEEPSEEK_MODEL\", \"deepseek/deepseek-chat\")\n",
    "TEMPERATURE = float(os.getenv(\"LLM_TEMPERATURE\", \"0.7\"))\n",
    "\n",
    "for task_name, task_info in TASKS.items():\n",
    "    run_experiment(\n",
    "        model_type=\"gemini\",\n",
    "        model_name=GEMINI_MODEL,\n",
    "        task_name=task_name,\n",
    "        task_info=task_info,\n",
    "        temperature=TEMPERATURE\n",
    "    )\n",
    "    time.sleep(1)\n",
    "\n",
    "for task_name, task_info in TASKS.items():\n",
    "    run_experiment(\n",
    "        model_type=\"deepseek\",\n",
    "        model_name=DEEPSEEK_MODEL,\n",
    "        task_name=task_name,\n",
    "        task_info=task_info,\n",
    "        temperature=TEMPERATURE\n",
    "    )\n",
    "    time.sleep(1)\n"
   ],
   "id": "runall",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
