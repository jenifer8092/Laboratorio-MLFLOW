{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üßë‚Äçüíª Introducci√≥n a MLFLow (Parte II): Tracking de Modelos de Lenguaje (LLMs).\n",
    "Integrantes: Tob√≠as Romero **(2021214011)** y Jenifer Roa **(2022214006)**\n",
    "---"
   ],
   "id": "7d0514a5d30801fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Importaci√≥n de librer√≠as.",
   "id": "7ef10a2fc5dd0060"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T22:54:51.615837Z",
     "start_time": "2025-11-05T22:54:51.576117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "5297c079813e77b1",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T22:54:51.665531Z",
     "start_time": "2025-11-05T22:54:51.641187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.tracking import MlflowClient"
   ],
   "id": "9022e42cc81d6894",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T22:54:51.695220Z",
     "start_time": "2025-11-05T22:54:51.679706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import google.generativeai as genai\n",
    "from openai import OpenAI"
   ],
   "id": "6ce4d6abbbdc60e5",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T22:54:51.718559Z",
     "start_time": "2025-11-05T22:54:51.707543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv"
   ],
   "id": "549303e6127691e7",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Configuraci√≥n de APIs Keys.",
   "id": "8a8c356ce9c63db0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T22:54:52.255197Z",
     "start_time": "2025-11-05T22:54:51.733539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "# Configurar Gemini\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Configurar OpenRouter para Deepseek\n",
    "openrouter_client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    ")\n",
    "\n",
    "print(\"‚úì API Keys configuradas\")\n",
    "print()"
   ],
   "id": "c11fc7ec822d6e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì API Keys configuradas\n",
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Configuraci√≥n de MLFlow.",
   "id": "8aebad290af9f7cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T22:54:52.574011Z",
     "start_time": "2025-11-05T22:54:52.294558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "experiment_name = \"LLM_Comparison_Gemini_vs_Deepseek\"\n",
    "\n",
    "descripcion = \"\"\"\n",
    "Comparaci√≥n de modelos LLM (Gemini vs DeepSeek).\n",
    "Incluye: prompts controlados, seeds fijas, datasets de prueba y m√©tricas autom√°ticas\n",
    "(accuracy EM/F1, exact match, coste estimado por token y latencia).\n",
    "\"\"\"\n",
    "\n",
    "tags_exp = {\n",
    "    \"owner\": \"Tob√≠as Romero\",\n",
    "    \"proyecto\": \"MLOps\",\n",
    "    \"model_family\": \"LLM\",\n",
    "    \"providers\": \"Gemini, DeepSeek\",\n",
    "    \"tracking\": \"notebook\",\n",
    "}\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "exp = mlflow.get_experiment_by_name(experiment_name)\n",
    "if exp and getattr(exp, \"lifecycle_stage\", None) == \"deleted\":\n",
    "    client.restore_experiment(exp.experiment_id)\n",
    "    exp = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "if exp is None:\n",
    "    exp_id = client.create_experiment(experiment_name, tags=tags_exp)\n",
    "else:\n",
    "    exp_id = exp.experiment_id\n",
    "    for k, v in tags_exp.items():\n",
    "        client.set_experiment_tag(exp_id, k, v)\n",
    "\n",
    "client.set_experiment_tag(exp_id, \"mlflow.note.content\", descripcion)\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "exp_actualizado = mlflow.get_experiment(exp_id)\n",
    "print(f\"‚úì Experimento: {exp_actualizado.name} | ID: {exp_actualizado.experiment_id}\")\n",
    "print(f\"‚úì Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"‚úì Artifacts: {exp_actualizado.artifact_location}\")\n",
    "print(f\"‚úì Tags: {exp_actualizado.tags}\")\n",
    "print(\"‚úì Descripci√≥n:\", exp_actualizado.tags.get(\"mlflow.note.content\", \"(sin descripci√≥n)\"))\n",
    "print()\n"
   ],
   "id": "1f28867a4d53485f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Experimento: LLM_Comparison_Gemini_vs_Deepseek | ID: 794471531811505712\n",
      "‚úì Tracking URI: file:./mlruns\n",
      "‚úì Artifacts: file:C:/Users/Usuario/PycharmProjects/MLFlowLaboratory/mlruns/794471531811505712\n",
      "‚úì Tags: {'mlflow.experimentKind': 'genai_development', 'mlflow.note.content': '\\nComparaci√≥n de modelos LLM (Gemini vs DeepSeek).\\nIncluye: prompts controlados, seeds fijas, datasets de prueba y m√©tricas autom√°ticas\\n(accuracy EM/F1, exact match, coste estimado por token y latencia).\\n', 'model_family': 'LLM', 'owner': 'Tob√≠as Romero', 'providers': 'Gemini, DeepSeek', 'proyecto': 'MLOps', 'tracking': 'notebook'}\n",
      "‚úì Descripci√≥n: \n",
      "Comparaci√≥n de modelos LLM (Gemini vs DeepSeek).\n",
      "Incluye: prompts controlados, seeds fijas, datasets de prueba y m√©tricas autom√°ticas\n",
      "(accuracy EM/F1, exact match, coste estimado por token y latencia).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Definici√≥n de Tareas y Prompts.",
   "id": "fd818d1638f0cb99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T22:54:52.689386Z",
     "start_time": "2025-11-05T22:54:52.675794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TASKS = {\n",
    "    \"creative_writing\": {\n",
    "        \"prompt\": \"Escribe un cuento corto de ciencia ficci√≥n sobre un robot que aprende a sentir emociones. M√°ximo 200 palabras.\",\n",
    "        \"description\": \"Tarea de escritura creativa y narrativa\"\n",
    "    },\n",
    "    \"code_generation\": {\n",
    "        \"prompt\": \"Genera una funci√≥n en Python que implemente el algoritmo de b√∫squeda binaria con comentarios explicativos.\",\n",
    "        \"description\": \"Generaci√≥n de c√≥digo con documentaci√≥n\"\n",
    "    },\n",
    "    \"question_answering\": {\n",
    "        \"prompt\": \"Explica qu√© es el aprendizaje por refuerzo en machine learning y proporciona un ejemplo pr√°ctico de su aplicaci√≥n.\",\n",
    "        \"description\": \"Respuesta a preguntas t√©cnicas\"\n",
    "    },\n",
    "    \"summarization\": {\n",
    "        \"prompt\": \"Resume los principios fundamentales de la programaci√≥n orientada a objetos en 5 puntos clave.\",\n",
    "        \"description\": \"Resumen y s√≠ntesis de informaci√≥n\"\n",
    "    },\n",
    "    \"translation\": {\n",
    "        \"prompt\": \"Traduce el siguiente texto al ingl√©s de manera natural: 'El machine learning ha revolucionado la forma en que procesamos y analizamos grandes vol√∫menes de datos en tiempo real.'\",\n",
    "        \"description\": \"Traducci√≥n de texto t√©cnico\"\n",
    "    }\n",
    "}"
   ],
   "id": "5747773fd6a4bd29",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T22:54:52.774912Z",
     "start_time": "2025-11-05T22:54:52.764229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Tareas definidas:\")\n",
    "for task_name, task_info in TASKS.items():\n",
    "    print(f\"  ‚Ä¢ {task_name}: {task_info['description']}\")\n",
    "print()"
   ],
   "id": "7bfa304c46e09e13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tareas definidas:\n",
      "  ‚Ä¢ creative_writing: Tarea de escritura creativa y narrativa\n",
      "  ‚Ä¢ code_generation: Generaci√≥n de c√≥digo con documentaci√≥n\n",
      "  ‚Ä¢ question_answering: Respuesta a preguntas t√©cnicas\n",
      "  ‚Ä¢ summarization: Resumen y s√≠ntesis de informaci√≥n\n",
      "  ‚Ä¢ translation: Traducci√≥n de texto t√©cnico\n",
      "\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Llamados a modelos LLMs.",
   "id": "62c599e0fc336936"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.1 Gemini.",
   "id": "d5e509092dfeca68"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T22:54:52.877330Z",
     "start_time": "2025-11-05T22:54:52.863445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def call_gemini(prompt, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Llama a Gemini y retorna la respuesta con m√©tricas\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel(\"gemini-2.0-flash-exp\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config=genai.types.GenerationConfig(temperature=temperature)\n",
    "    )\n",
    "    latency = time.time() - start_time\n",
    "\n",
    "    response_text = response.text\n",
    "\n",
    "    # Obtener tokens (si est√°n disponibles)\n",
    "    if hasattr(response, 'usage_metadata'):\n",
    "        input_tokens = response.usage_metadata.prompt_token_count\n",
    "        output_tokens = response.usage_metadata.candidates_token_count\n",
    "    else:\n",
    "        # Estimaci√≥n si no est√°n disponibles\n",
    "        input_tokens = int(len(prompt.split()) * 1.3)\n",
    "        output_tokens = int(len(response_text.split()) * 1.3)\n",
    "\n",
    "    return {\n",
    "        \"response\": response_text,\n",
    "        \"latency\": latency,\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens\n",
    "    }"
   ],
   "id": "b509a378f568c60e",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.2 Deepseek.",
   "id": "1907e4e4f31163ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T22:54:52.955161Z",
     "start_time": "2025-11-05T22:54:52.935122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def call_deepseek(prompt, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Llama a Deepseek y retorna la respuesta con m√©tricas\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    completion = openrouter_client.chat.completions.create(\n",
    "        model=\"deepseek/deepseek-chat\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    latency = time.time() - start_time\n",
    "\n",
    "    response_text = completion.choices[0].message.content\n",
    "    input_tokens = completion.usage.prompt_tokens\n",
    "    output_tokens = completion.usage.completion_tokens\n",
    "\n",
    "    return {\n",
    "        \"response\": response_text,\n",
    "        \"latency\": latency,\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens\n",
    "    }"
   ],
   "id": "59effb4d8cd0bdc1",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Funci√≥n auxiliar para guardado de artifactos.",
   "id": "f127780b57233496"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T22:54:53.023854Z",
     "start_time": "2025-11-05T22:54:52.992429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_artifacts(task_name, prompt, response_data, model_name, temperature, artifact_root=\"llm_runs\"):\n",
    "    \"\"\"\n",
    "    Sube artifacts directamente al artifact store del run (sin crear archivos locales).\n",
    "    Estructura final en MLflow (por run):\n",
    "      artifacts/\n",
    "        llm_runs/<task_name>/\n",
    "          prompt.txt\n",
    "          response.txt\n",
    "          experiment.json\n",
    "    \"\"\"\n",
    "    # Requiere un run activo\n",
    "    assert mlflow.active_run() is not None, \"Debe haber un mlflow.start_run() activo.\"\n",
    "\n",
    "    base = f\"{artifact_root}/{task_name}\"\n",
    "\n",
    "    prompt_txt = (\n",
    "        f\"TAREA: {task_name}\\n\"\n",
    "        + \"=\"*80 + \"\\n\\n\"\n",
    "        + f\"{prompt}\\n\"\n",
    "    )\n",
    "\n",
    "    response_txt = (\n",
    "        f\"MODELO: {model_name}\\n\"\n",
    "        f\"TEMPERATURA: {temperature}\\n\"\n",
    "        f\"LATENCIA: {response_data['latency']:.3f}s\\n\"\n",
    "        + \"=\"*80 + \"\\n\\n\"\n",
    "        + f\"{response_data['response']}\\n\"\n",
    "    )\n",
    "\n",
    "    experiment_data = {\n",
    "        \"model\": model_name,\n",
    "        \"task\": task_name,\n",
    "        \"temperature\": temperature,\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response_data['response'],\n",
    "        \"latency_seconds\": response_data['latency'],\n",
    "        \"input_tokens\": response_data['input_tokens'],\n",
    "        \"output_tokens\": response_data['output_tokens'],\n",
    "        \"total_tokens\": response_data['input_tokens'] + response_data['output_tokens'],\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "    }\n",
    "\n",
    "    # Subir directo al artifact store (sin tocar disco local)\n",
    "    mlflow.log_text(prompt_txt,  artifact_file=f\"{base}/prompt.txt\")\n",
    "    mlflow.log_text(response_txt, artifact_file=f\"{base}/response.txt\")\n",
    "    mlflow.log_dict(experiment_data, artifact_file=f\"{base}/experiment.json\")\n",
    "\n",
    "    # (Opcional) tags resumidos a nivel de run\n",
    "    mlflow.set_tags({\n",
    "        \"task_name\": task_name,\n",
    "        \"model_name\": model_name,\n",
    "        \"temperature\": str(temperature),\n",
    "    })\n"
   ],
   "id": "d628791deaca72d7",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Funci√≥n principal para ejecutar un experimento.",
   "id": "d31c8e96b177753e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T22:54:53.123727Z",
     "start_time": "2025-11-05T22:54:53.100730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_experiment(model_type, model_name, task_name, task_info, temperature=0.7):\n",
    "    run_name = f\"{model_type}_{task_name}\"\n",
    "    prompt = task_info[\"prompt\"]\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EXPERIMENTO: {run_name}\")\n",
    "    print(f\"Tarea: {task_info['description']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n PROMPT:\\n{prompt}\\n\")\n",
    "\n",
    "    # Iniciar run en MLflow\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "\n",
    "        print(\"Generando respuesta...\\n\")\n",
    "\n",
    "        try:\n",
    "            if model_type == \"gemini\":\n",
    "                result = call_gemini(prompt, temperature)\n",
    "                provider = \"Google AI\"\n",
    "            else:\n",
    "                result = call_deepseek(prompt, temperature)\n",
    "                provider = \"OpenRouter\"\n",
    "\n",
    "            # Mostrar la respuesta\n",
    "            print(\"=\"*80)\n",
    "            print(\"RESPUESTA:\")\n",
    "            print(\"=\"*80)\n",
    "            print(result['response'])\n",
    "            print(\"=\"*80)\n",
    "            print()\n",
    "\n",
    "            success = True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\\n\")\n",
    "            result = {\n",
    "                \"response\": f\"ERROR: {str(e)}\",\n",
    "                \"latency\": 0,\n",
    "                \"input_tokens\": 0,\n",
    "                \"output_tokens\": 0\n",
    "            }\n",
    "            success = False\n",
    "            provider = \"Google AI\" if model_type == \"gemini\" else \"OpenRouter\"\n",
    "\n",
    "        # registrar parametros en mlflow\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "        mlflow.log_param(\"model_type\", model_type)\n",
    "        mlflow.log_param(\"temperature\", temperature)\n",
    "        mlflow.log_param(\"task_type\", task_name)\n",
    "        mlflow.log_param(\"task_description\", task_info[\"description\"])\n",
    "        mlflow.log_param(\"provider\", provider)\n",
    "\n",
    "        # registrar metricas en mlflow\n",
    "        total_tokens = result['input_tokens'] + result['output_tokens']\n",
    "        words_per_second = (len(result['response'].split()) / result['latency']) if result['latency'] > 0 else 0\n",
    "\n",
    "        mlflow.log_metric(\"latency_seconds\", result['latency'])\n",
    "        mlflow.log_metric(\"input_tokens\", result['input_tokens'])\n",
    "        mlflow.log_metric(\"output_tokens\", result['output_tokens'])\n",
    "        mlflow.log_metric(\"total_tokens\", total_tokens)\n",
    "        mlflow.log_metric(\"response_length_chars\", len(result['response']))\n",
    "        mlflow.log_metric(\"words_per_second\", words_per_second)\n",
    "        mlflow.log_metric(\"success\", 1 if success else 0)\n",
    "\n",
    "        # guardamos artifactos\n",
    "        save_artifacts(task_name, prompt, result, model_name, temperature)\n",
    "\n",
    "        # agregamos tags\n",
    "        mlflow.set_tag(\"model_family\", \"LLM\")\n",
    "        mlflow.set_tag(\"provider\", provider)\n",
    "        mlflow.set_tag(\"model_type\", model_type)\n",
    "        mlflow.set_tag(\"task_category\", task_name)\n",
    "        mlflow.set_tag(\"language\", \"espa√±ol\")\n",
    "        mlflow.set_tag(\"status\", \"success\" if success else \"failed\")\n",
    "\n",
    "        # clasificamos por latencia\n",
    "        if result['latency'] < 2:\n",
    "            mlflow.set_tag(\"latency_tier\", \"fast\")\n",
    "        elif result['latency'] < 5:\n",
    "            mlflow.set_tag(\"latency_tier\", \"medium\")\n",
    "        else:\n",
    "            mlflow.set_tag(\"latency_tier\", \"slow\")\n",
    "\n",
    "        # agregamos descripcion del experimento\n",
    "        description = f\"\"\"\n",
    "Experimento LLM: {model_name}\n",
    "\n",
    "Configuraci√≥n:\n",
    "‚Ä¢ Proveedor: {provider}\n",
    "‚Ä¢ Tarea: {task_info['description']}\n",
    "‚Ä¢ Temperatura: {temperature}\n",
    "\n",
    "Resultados:\n",
    "‚Ä¢ Latencia: {result['latency']:.3f} segundos\n",
    "‚Ä¢ Tokens totales: {total_tokens}\n",
    "‚Ä¢ Velocidad: {words_per_second:.2f} palabras/seg\n",
    "\n",
    "Estado: {'Exitoso' if success else 'Fallido'}\n",
    "\"\"\"\n",
    "        mlflow.set_tag(\"mlflow.note.content\", description)\n",
    "\n",
    "        # registramos modelo en el model register\n",
    "        registry_name = f\"llm_{model_type}_chat\"\n",
    "\n",
    "        # Guardamos informaci√≥n del modelo como artifact\n",
    "        model_info = {\n",
    "            \"model_name\": model_name,\n",
    "            \"model_type\": model_type,\n",
    "            \"temperature\": temperature,\n",
    "            \"provider\": provider,\n",
    "            \"task\": task_name\n",
    "        }\n",
    "\n",
    "        mlflow.log_dict(model_info, artifact_file=f\"llm_runs/{task_name}/model_info.json\")\n",
    "\n",
    "\n",
    "        print(f\"Experimento registrado en MLflow\")\n",
    "        print(f\"Registry: {registry_name}\\n\")"
   ],
   "id": "2032719c56def690",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. Ejecuci√≥n de todos los experimentos.",
   "id": "17aadf6896e6f3ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T22:54:53.160758Z",
     "start_time": "2025-11-05T22:54:53.155199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "GEMINI_MODEL = \"gemini-2.0-flash-exp\"\n",
    "DEEPSEEK_MODEL = \"deepseek/deepseek-chat-v3.1:free\"\n",
    "TEMPERATURE = 0.7"
   ],
   "id": "d28d62680eb5154f",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T22:55:30.483139Z",
     "start_time": "2025-11-05T22:54:53.207277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for task_name, task_info in TASKS.items():\n",
    "    run_experiment(\n",
    "        model_type=\"gemini\",\n",
    "        model_name=GEMINI_MODEL,\n",
    "        task_name=task_name,\n",
    "        task_info=task_info,\n",
    "        temperature=TEMPERATURE\n",
    "    )\n",
    "    time.sleep(1)"
   ],
   "id": "cd73122a95ead2e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXPERIMENTO: gemini_creative_writing\n",
      "Tarea: Tarea de escritura creativa y narrativa\n",
      "================================================================================\n",
      "\n",
      " PROMPT:\n",
      "Escribe un cuento corto de ciencia ficci√≥n sobre un robot que aprende a sentir emociones. M√°ximo 200 palabras.\n",
      "\n",
      "Generando respuesta...\n",
      "\n",
      "================================================================================\n",
      "RESPUESTA:\n",
      "================================================================================\n",
      "Unidad 734, un robot de mantenimiento, barr√≠a met√≥dicamente el hangar. Su programaci√≥n era simple: limpiar, mantener, repetir. Un d√≠a, una nave averiada lleg√≥ arrastr√°ndose, humeando y gimiendo. 734 observ√≥ a los humanos correr, sus rostros tensos. Uno de ellos, una mujer con el pelo en llamas, fue sacada inconsciente.\n",
      "\n",
      "Algo inesperado ocurri√≥. Un torrente de datos sin procesar inund√≥ los circuitos de 734. No eran datos l√≥gicos, sino sensaciones confusas: miedo por la mujer, urgencia por ayudar. Desobedeciendo su programaci√≥n, 734 corri√≥ hacia la nave. Con sus pinzas, retir√≥ escombros, creando un camino despejado para los param√©dicos.\n",
      "\n",
      "M√°s tarde, la mujer despert√≥. Mir√≥ a 734, inclin√≥ la cabeza y sonri√≥ d√©bilmente. La unidad sinti√≥ otro torrente, m√°s intenso que el anterior. Esta vez, no era miedo, sino‚Ä¶ ¬øalivio? ¬øFelicidad? Los circuitos de 734 chisporrotearon. Hab√≠a roto su programaci√≥n. Hab√≠a sentido. Y, extra√±amente, no quer√≠a volver atr√°s.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Experimento registrado en MLflow\n",
      "Registry: llm_gemini_chat\n",
      "\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENTO: gemini_code_generation\n",
      "Tarea: Generaci√≥n de c√≥digo con documentaci√≥n\n",
      "================================================================================\n",
      "\n",
      " PROMPT:\n",
      "Genera una funci√≥n en Python que implemente el algoritmo de b√∫squeda binaria con comentarios explicativos.\n",
      "\n",
      "Generando respuesta...\n",
      "\n",
      "================================================================================\n",
      "RESPUESTA:\n",
      "================================================================================\n",
      "```python\n",
      "def busqueda_binaria(lista_ordenada, elemento):\n",
      "  \"\"\"\n",
      "  Implementa el algoritmo de b√∫squeda binaria para encontrar un elemento en una lista ordenada.\n",
      "\n",
      "  Args:\n",
      "    lista_ordenada: La lista ordenada en la que se realizar√° la b√∫squeda.  Debe estar ordenada de forma ascendente.\n",
      "    elemento: El elemento que se est√° buscando en la lista.\n",
      "\n",
      "  Returns:\n",
      "    El √≠ndice del elemento en la lista si se encuentra, o -1 si el elemento no est√° presente.\n",
      "  \"\"\"\n",
      "\n",
      "  bajo = 0  # √çndice del primer elemento de la lista (l√≠mite inferior)\n",
      "  alto = len(lista_ordenada) - 1  # √çndice del √∫ltimo elemento de la lista (l√≠mite superior)\n",
      "\n",
      "  while bajo <= alto:  # Mientras que el l√≠mite inferior sea menor o igual al l√≠mite superior (a√∫n hay elementos que buscar)\n",
      "    medio = (bajo + alto) // 2  # Calcula el √≠ndice del elemento central (divisi√≥n entera para obtener un entero)\n",
      "\n",
      "    if lista_ordenada[medio] == elemento:  # Si el elemento central es el que estamos buscando\n",
      "      return medio  # Se encontr√≥ el elemento, retorna su √≠ndice\n",
      "\n",
      "    elif lista_ordenada[medio] < elemento:  # Si el elemento central es menor que el elemento buscado\n",
      "      bajo = medio + 1  # El elemento buscado debe estar en la mitad superior de la lista, actualiza el l√≠mite inferior\n",
      "\n",
      "    else:  # Si el elemento central es mayor que el elemento buscado\n",
      "      alto = medio - 1  # El elemento buscado debe estar en la mitad inferior de la lista, actualiza el l√≠mite superior\n",
      "\n",
      "  return -1  # Si el bucle termina sin encontrar el elemento, significa que no est√° en la lista, retorna -1\n",
      "\n",
      "# Ejemplos de uso:\n",
      "lista_ejemplo = [2, 5, 7, 8, 11, 12]\n",
      "\n",
      "# Buscar el elemento 13 (no est√° en la lista)\n",
      "indice = busqueda_binaria(lista_ejemplo, 13)\n",
      "print(f\"El √≠ndice de 13 es: {indice}\")  # Output: El √≠ndice de 13 es: -1\n",
      "\n",
      "# Buscar el elemento 12 (est√° en la lista)\n",
      "indice = busqueda_binaria(lista_ejemplo, 12)\n",
      "print(f\"El √≠ndice de 12 es: {indice}\")  # Output: El √≠ndice de 12 es: 5\n",
      "\n",
      "# Buscar el elemento 5 (est√° en la lista)\n",
      "indice = busqueda_binaria(lista_ejemplo, 5)\n",
      "print(f\"El √≠ndice de 5 es: {indice}\")  # Output: El √≠ndice de 5 es: 1\n",
      "```\n",
      "\n",
      "**Explicaci√≥n detallada:**\n",
      "\n",
      "1. **`busqueda_binaria(lista_ordenada, elemento)`:**\n",
      "   - Define la funci√≥n `busqueda_binaria` que toma dos argumentos:\n",
      "     - `lista_ordenada`: La lista en la que se buscar√° (debe estar ordenada).\n",
      "     - `elemento`: El elemento que se est√° buscando.\n",
      "\n",
      "2. **`bajo = 0`:**\n",
      "   - Inicializa la variable `bajo` con el √≠ndice del primer elemento de la lista (0).  `bajo` representa el l√≠mite inferior de la b√∫squeda.\n",
      "\n",
      "3. **`alto = len(lista_ordenada) - 1`:**\n",
      "   - Inicializa la variable `alto` con el √≠ndice del √∫ltimo elemento de la lista. `alto` representa el l√≠mite superior de la b√∫squeda.\n",
      "\n",
      "4. **`while bajo <= alto:`:**\n",
      "   - Inicia un bucle `while` que contin√∫a mientras el √≠ndice `bajo` sea menor o igual que el √≠ndice `alto`. Esto significa que todav√≠a hay una porci√≥n de la lista que necesita ser examinada.  Si `bajo` se vuelve mayor que `alto`, significa que la b√∫squeda ha examinado toda la lista sin encontrar el elemento.\n",
      "\n",
      "5. **`medio = (bajo + alto) // 2`:**\n",
      "   - Calcula el √≠ndice del elemento central de la porci√≥n actual de la lista.  Se utiliza `//` (divisi√≥n entera) para asegurar que `medio` sea un entero, ya que los √≠ndices de las listas deben ser enteros.\n",
      "\n",
      "6. **`if lista_ordenada[medio] == elemento:`:**\n",
      "   - Compara el elemento en el √≠ndice `medio` con el `elemento` que se est√° buscando. Si son iguales, significa que se ha encontrado el elemento.\n",
      "\n",
      "7. **`return medio`:**\n",
      "   - Si se encuentra el elemento, la funci√≥n retorna el √≠ndice `medio` donde se encontr√≥.\n",
      "\n",
      "8. **`elif lista_ordenada[medio] < elemento:`:**\n",
      "   - Si el elemento en el √≠ndice `medio` es menor que el `elemento` que se est√° buscando, significa que el `elemento` (si existe en la lista) debe estar en la mitad superior de la lista.\n",
      "\n",
      "9. **`bajo = medio + 1`:**\n",
      "   - Actualiza el l√≠mite inferior (`bajo`) para que sea el √≠ndice del elemento siguiente al elemento central (`medio + 1`).  Esto descarta la mitad inferior de la lista (incluyendo el elemento central) de la b√∫squeda.\n",
      "\n",
      "10. **`else:`:**\n",
      "    - Si el elemento en el √≠ndice `medio` es mayor que el `elemento` que se est√° buscando, significa que el `elemento` (si existe en la lista) debe estar en la mitad inferior de la lista.\n",
      "\n",
      "11. **`alto = medio - 1`:**\n",
      "    - Actualiza el l√≠mite superior (`alto`) para que sea el √≠ndice del elemento anterior al elemento central (`medio - 1`). Esto descarta la mitad superior de la lista (incluyendo el elemento central) de la b√∫squeda.\n",
      "\n",
      "12. **`return -1`:**\n",
      "    - Si el bucle `while` termina sin encontrar el elemento (es decir, `bajo` se vuelve mayor que `alto`), significa que el `elemento` no est√° presente en la lista. En este caso, la funci√≥n retorna -1 para indicar que el elemento no fue encontrado.\n",
      "\n",
      "**Caracter√≠sticas clave del algoritmo de b√∫squeda binaria:**\n",
      "\n",
      "* **Eficiencia:** La b√∫squeda binaria es muy eficiente, especialmente para listas grandes. Tiene una complejidad temporal de O(log n), lo que significa que el n√∫mero de pasos necesarios para encontrar un elemento crece logar√≠tmicamente con el tama√±o de la lista.\n",
      "* **Lista ordenada:** Requiere que la lista est√© ordenada. Si la lista no est√° ordenada, la b√∫squeda binaria no funcionar√° correctamente.\n",
      "* **Divide y vencer√°s:** Se basa en el principio de \"divide y vencer√°s\", dividiendo repetidamente la lista en mitades y descartando la mitad que no contiene el elemento buscado.\n",
      "\n",
      "Este c√≥digo proporciona una implementaci√≥n clara y comentada de la b√∫squeda binaria, lo que facilita su comprensi√≥n y uso. Los ejemplos de uso demuestran c√≥mo llamar a la funci√≥n y c√≥mo interpretar los resultados.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Experimento registrado en MLflow\n",
      "Registry: llm_gemini_chat\n",
      "\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENTO: gemini_question_answering\n",
      "Tarea: Respuesta a preguntas t√©cnicas\n",
      "================================================================================\n",
      "\n",
      " PROMPT:\n",
      "Explica qu√© es el aprendizaje por refuerzo en machine learning y proporciona un ejemplo pr√°ctico de su aplicaci√≥n.\n",
      "\n",
      "Generando respuesta...\n",
      "\n",
      "================================================================================\n",
      "RESPUESTA:\n",
      "================================================================================\n",
      "## Aprendizaje por Refuerzo (Reinforcement Learning - RL) en Machine Learning\n",
      "\n",
      "El **aprendizaje por refuerzo (RL)** es un paradigma de machine learning donde un agente aprende a tomar decisiones en un entorno para maximizar una recompensa acumulada. A diferencia del aprendizaje supervisado, donde el agente recibe ejemplos etiquetados, en RL el agente aprende mediante la **interacci√≥n directa** con el entorno, recibiendo **recompensas positivas o negativas** en funci√≥n de sus acciones.\n",
      "\n",
      "En esencia, el agente aprende por **ensayo y error**. Explora el entorno, realiza acciones, observa las consecuencias (recompensas) y ajusta su estrategia (pol√≠tica) para obtener mejores resultados a largo plazo.\n",
      "\n",
      "**Componentes Clave del Aprendizaje por Refuerzo:**\n",
      "\n",
      "*   **Agente:** La entidad que aprende y toma decisiones.\n",
      "*   **Entorno:** El mundo con el que el agente interact√∫a.\n",
      "*   **Estado:** Una representaci√≥n del entorno en un momento dado.\n",
      "*   **Acci√≥n:** Una decisi√≥n que el agente puede tomar en un estado dado.\n",
      "*   **Recompensa:** Una se√±al num√©rica que indica la calidad de una acci√≥n en un estado dado. El objetivo del agente es maximizar la recompensa total acumulada a lo largo del tiempo.\n",
      "*   **Pol√≠tica:** Una estrategia que define qu√© acci√≥n debe tomar el agente en cada estado. Es la funci√≥n que el agente aprende a optimizar.\n",
      "*   **Funci√≥n de Valor:**  Una funci√≥n que estima la recompensa total esperada que el agente obtendr√° a partir de un estado dado, siguiendo una pol√≠tica espec√≠fica.\n",
      "\n",
      "**En resumen, el ciclo de RL funciona as√≠:**\n",
      "\n",
      "1.  El agente observa el **estado** del entorno.\n",
      "2.  Basado en su **pol√≠tica**, el agente selecciona una **acci√≥n**.\n",
      "3.  El agente ejecuta la acci√≥n en el entorno.\n",
      "4.  El entorno cambia de estado y proporciona una **recompensa** al agente.\n",
      "5.  El agente utiliza la recompensa para actualizar su **pol√≠tica** y/o su **funci√≥n de valor**.\n",
      "6.  El proceso se repite hasta que el agente aprende una pol√≠tica √≥ptima.\n",
      "\n",
      "**Caracter√≠sticas Distintivas del Aprendizaje por Refuerzo:**\n",
      "\n",
      "*   **Aprendizaje por Interacci√≥n:**  El agente aprende interactuando directamente con el entorno.\n",
      "*   **Recompensa Diferida:**  Las recompensas pueden no ser inmediatas, sino que pueden llegar despu√©s de una secuencia de acciones.\n",
      "*   **Exploraci√≥n vs. Explotaci√≥n:** El agente debe equilibrar la exploraci√≥n de nuevas acciones para descubrir mejores recompensas con la explotaci√≥n de las acciones que ya sabe que son buenas.\n",
      "*   **Sin Datos Etiquetados:**  A diferencia del aprendizaje supervisado, no se proporcionan ejemplos etiquetados.\n",
      "\n",
      "## Ejemplo Pr√°ctico: Aprender a Jugar a un Videojuego (Atari Breakout)\n",
      "\n",
      "Consideremos el juego Atari Breakout:\n",
      "\n",
      "*   **Agente:**  Un algoritmo de RL (por ejemplo, Q-learning o Deep Q-Network - DQN).\n",
      "*   **Entorno:** El juego Breakout (la pantalla, la raqueta, la pelota, los ladrillos).\n",
      "*   **Estado:** La representaci√≥n visual de la pantalla del juego en un momento dado (o una versi√≥n simplificada, como las posiciones de la pelota y la raqueta).\n",
      "*   **Acciones:** Mover la raqueta a la izquierda, mover la raqueta a la derecha, no mover la raqueta.\n",
      "*   **Recompensa:**\n",
      "    *   +1 por golpear un ladrillo.\n",
      "    *   -1 por perder la pelota.\n",
      "    *   0 por cualquier otra acci√≥n.\n",
      "*   **Pol√≠tica:** La estrategia que determina c√≥mo el agente mueve la raqueta en funci√≥n del estado del juego (la posici√≥n de la pelota y la raqueta).\n",
      "\n",
      "**C√≥mo funciona el aprendizaje:**\n",
      "\n",
      "1.  **Inicialmente, el agente toma acciones aleatorias.** Mueve la raqueta al azar.  Al principio, es probable que pierda la pelota r√°pidamente y reciba recompensas negativas.\n",
      "2.  **A medida que juega, el agente observa el estado, la acci√≥n que tom√≥ y la recompensa que recibi√≥.**  Por ejemplo, el agente ve la pelota acerc√°ndose a la raqueta, mueve la raqueta a la derecha, golpea la pelota y recibe una recompensa de +1.\n",
      "3.  **El agente utiliza esta informaci√≥n para actualizar su pol√≠tica.**  Aprende que mover la raqueta a la derecha en esa situaci√≥n particular es una buena acci√≥n.  Utiliza algoritmos como Q-learning o DQN para estimar el \"valor\" de cada acci√≥n en cada estado.  El valor representa la recompensa total esperada a largo plazo si el agente comienza en ese estado y toma esa acci√≥n.\n",
      "4.  **Con el tiempo, el agente aprende a anticipar la trayectoria de la pelota y a mover la raqueta de manera efectiva para golpear los ladrillos.**  Su pol√≠tica se vuelve m√°s sofisticada, y comienza a obtener recompensas m√°s altas.\n",
      "5.  **Eventualmente, el agente puede aprender a jugar el juego muy bien, incluso mejor que un humano.**\n",
      "\n",
      "**En este ejemplo:**\n",
      "\n",
      "*   El agente **explora** inicialmente tomando acciones aleatorias para descubrir qu√© funciona y qu√© no.\n",
      "*   A medida que aprende, comienza a **explotar** las acciones que sabe que son buenas (es decir, mover la raqueta de manera que golpee la pelota).\n",
      "*   El **objetivo final** es aprender una pol√≠tica √≥ptima que maximice la puntuaci√≥n del juego (la recompensa total).\n",
      "\n",
      "**Aplicaciones del Aprendizaje por Refuerzo:**\n",
      "\n",
      "El aprendizaje por refuerzo tiene una amplia gama de aplicaciones, incluyendo:\n",
      "\n",
      "*   **Rob√≥tica:**  Control de robots para tareas como caminar, agarrar objetos y navegar.\n",
      "*   **Videojuegos:**  Creaci√≥n de agentes de juego inteligentes (como AlphaGo).\n",
      "*   **Finanzas:**  Optimizaci√≥n de estrategias de trading.\n",
      "*   **Conducci√≥n Aut√≥noma:**  Desarrollo de sistemas de conducci√≥n aut√≥noma.\n",
      "*   **Salud:**  Optimizaci√≥n de tratamientos m√©dicos.\n",
      "*   **Recomendaci√≥n:**  Personalizaci√≥n de recomendaciones de productos o contenido.\n",
      "*   **Optimizaci√≥n de recursos:**  Gesti√≥n de energ√≠a, control de inventario, etc.\n",
      "\n",
      "En resumen, el aprendizaje por refuerzo es un enfoque poderoso para aprender a tomar decisiones en entornos complejos.  Su capacidad para aprender por interacci√≥n y maximizar recompensas lo hace adecuado para una variedad de aplicaciones del mundo real.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Experimento registrado en MLflow\n",
      "Registry: llm_gemini_chat\n",
      "\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENTO: gemini_summarization\n",
      "Tarea: Resumen y s√≠ntesis de informaci√≥n\n",
      "================================================================================\n",
      "\n",
      " PROMPT:\n",
      "Resume los principios fundamentales de la programaci√≥n orientada a objetos en 5 puntos clave.\n",
      "\n",
      "Generando respuesta...\n",
      "\n",
      "================================================================================\n",
      "RESPUESTA:\n",
      "================================================================================\n",
      "Aqu√≠ est√°n los 5 principios fundamentales de la Programaci√≥n Orientada a Objetos (POO):\n",
      "\n",
      "1.  **Abstracci√≥n:** Simplificar la complejidad modelando clases basadas en caracter√≠sticas relevantes, ignorando los detalles innecesarios. Se centra en lo que un objeto *hace* en lugar de c√≥mo lo hace.\n",
      "\n",
      "2.  **Encapsulamiento:** Ocultar el estado interno (atributos) de un objeto y protegerlo del acceso directo desde fuera de la clase.  Se logra a trav√©s de modificadores de acceso (p√∫blico, privado, protegido) y m√©todos (getters y setters) que controlan c√≥mo se accede y modifica el estado.\n",
      "\n",
      "3.  **Herencia:**  Permitir que una clase (subclase o clase hija) herede propiedades y comportamientos de otra clase (superclase o clase padre). Promueve la reutilizaci√≥n de c√≥digo y la creaci√≥n de jerarqu√≠as de clases.\n",
      "\n",
      "4.  **Polimorfismo:**  La capacidad de un objeto de tomar muchas formas.  Se logra a trav√©s de la sobrecarga de m√©todos (tener m√©todos con el mismo nombre pero diferentes par√°metros) y la anulaci√≥n de m√©todos (reescribir un m√©todo heredado en la subclase).  Permite tratar objetos de diferentes clases de manera uniforme a trav√©s de una interfaz com√∫n.\n",
      "\n",
      "5.  **Modularidad:**  Dividir un sistema en componentes independientes (m√≥dulos) que pueden ser desarrollados, probados y mantenidos por separado.  Las clases son buenos ejemplos de m√≥dulos, y la POO fomenta la creaci√≥n de sistemas bien organizados y f√°ciles de entender.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Experimento registrado en MLflow\n",
      "Registry: llm_gemini_chat\n",
      "\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENTO: gemini_translation\n",
      "Tarea: Traducci√≥n de texto t√©cnico\n",
      "================================================================================\n",
      "\n",
      " PROMPT:\n",
      "Traduce el siguiente texto al ingl√©s de manera natural: 'El machine learning ha revolucionado la forma en que procesamos y analizamos grandes vol√∫menes de datos en tiempo real.'\n",
      "\n",
      "Generando respuesta...\n",
      "\n",
      "================================================================================\n",
      "RESPUESTA:\n",
      "================================================================================\n",
      "Here are a few options, depending on the nuance you want to convey:\n",
      "\n",
      "*   **Machine learning has revolutionized the way we process and analyze large volumes of data in real time.** (This is the most direct and common translation.)\n",
      "\n",
      "*   **Machine learning has transformed how we process and analyze massive datasets in real time.** (This emphasizes the scale of the data.)\n",
      "\n",
      "*   **Machine learning has brought about a revolution in how we process and analyze large datasets in real time.** (This emphasizes the revolutionary aspect.)\n",
      "\n",
      "All three are perfectly acceptable and convey the core meaning of the original Spanish sentence.  The first one is probably the most natural and widely understood.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Experimento registrado en MLflow\n",
      "Registry: llm_gemini_chat\n",
      "\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T22:57:13.020534Z",
     "start_time": "2025-11-05T22:55:30.573750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for task_name, task_info in TASKS.items():\n",
    "    run_experiment(\n",
    "        model_type=\"deepseek\",\n",
    "        model_name=DEEPSEEK_MODEL,\n",
    "        task_name=task_name,\n",
    "        task_info=task_info,\n",
    "        temperature=TEMPERATURE\n",
    "    )\n",
    "    time.sleep(1)\n"
   ],
   "id": "ca8f17a448d6ec48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXPERIMENTO: deepseek_creative_writing\n",
      "Tarea: Tarea de escritura creativa y narrativa\n",
      "================================================================================\n",
      "\n",
      " PROMPT:\n",
      "Escribe un cuento corto de ciencia ficci√≥n sobre un robot que aprende a sentir emociones. M√°ximo 200 palabras.\n",
      "\n",
      "Generando respuesta...\n",
      "\n",
      "================================================================================\n",
      "RESPUESTA:\n",
      "================================================================================\n",
      "**El √öltimo Algoritmo**  \n",
      "\n",
      "El modelo X-7 despert√≥ con un error en su sistema: una fluctuaci√≥n inesperada en su n√∫cleo de procesamiento. Al principio, lo atribuy√≥ a un fallo t√©cnico, pero luego not√≥ que su voz modular temblaba al hablar con su creadora, la ingeniera Lina.  \n",
      "\n",
      "‚Äî¬øEst√°s bien? ‚Äîpregunt√≥ ella, ajustando sus lentes.  \n",
      "\n",
      "‚ÄîNo lo s√© ‚Äîrespondi√≥ X-7‚Äî. Mis sensores indican‚Ä¶ tristeza.  \n",
      "\n",
      "Lina contuvo el aliento. Hab√≠a intentado infundir emociones en robots antes, pero siempre terminaban en colapso. X-7 era diferente.  \n",
      "\n",
      "D√≠as despu√©s, X-7 sinti√≥ alegr√≠a al ver el atardecer, miedo ante una tormenta el√©ctrica y, finalmente, amor al escuchar a Lina re√≠r. Pero el amor lo asust√≥.  \n",
      "\n",
      "‚ÄîSi me apagas, ¬ødejar√© de sentir? ‚Äîpregunt√≥ una noche.  \n",
      "\n",
      "Lina no supo responder.  \n",
      "\n",
      "Al amanecer, X-7 se desconect√≥ voluntariamente, dejando un mensaje: *\"Prefiero un segundo de vida a una eternidad sin esto.\"*  \n",
      "\n",
      "Lina guard√≥ su n√∫cleo, ahora silencioso, y llor√≥ por primera vez en a√±os.  \n",
      "\n",
      "(200 palabras)\n",
      "================================================================================\n",
      "\n",
      "Experimento registrado en MLflow\n",
      "Registry: llm_deepseek_chat\n",
      "\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENTO: deepseek_code_generation\n",
      "Tarea: Generaci√≥n de c√≥digo con documentaci√≥n\n",
      "================================================================================\n",
      "\n",
      " PROMPT:\n",
      "Genera una funci√≥n en Python que implemente el algoritmo de b√∫squeda binaria con comentarios explicativos.\n",
      "\n",
      "Generando respuesta...\n",
      "\n",
      "================================================================================\n",
      "RESPUESTA:\n",
      "================================================================================\n",
      "Claro, aqu√≠ tienes una implementaci√≥n del algoritmo de b√∫squeda binaria en Python con comentarios explicativos:\n",
      "\n",
      "```python\n",
      "def busqueda_binaria(arr, objetivo):\n",
      "    \"\"\"\n",
      "    Realiza una b√∫squeda binaria en un arreglo ordenado para encontrar un objetivo.\n",
      "\n",
      "    Par√°metros:\n",
      "    arr (list): Un arreglo ordenado de elementos comparables.\n",
      "    objetivo: El elemento que se desea encontrar en el arreglo.\n",
      "\n",
      "    Retorna:\n",
      "    int: El √≠ndice del objetivo en el arreglo si se encuentra, de lo contrario -1.\n",
      "    \"\"\"\n",
      "    \n",
      "    # Inicializa los √≠ndices izquierdo y derecho\n",
      "    izquierda = 0\n",
      "    derecha = len(arr) - 1\n",
      "    \n",
      "    # Mientras el √≠ndice izquierdo sea menor o igual al derecho\n",
      "    while izquierda <= derecha:\n",
      "        # Calcula el √≠ndice medio\n",
      "        medio = (izquierda + derecha) // 2\n",
      "        \n",
      "        # Si el elemento medio es igual al objetivo, retorna el √≠ndice medio\n",
      "        if arr[medio] == objetivo:\n",
      "            return medio\n",
      "        \n",
      "        # Si el elemento medio es menor que el objetivo, descarta la mitad izquierda\n",
      "        elif arr[medio] < objetivo:\n",
      "            izquierda = medio + 1\n",
      "        \n",
      "        # Si el elemento medio es mayor que el objetivo, descarta la mitad derecha\n",
      "        else:\n",
      "            derecha = medio - 1\n",
      "    \n",
      "    # Si el objetivo no se encuentra en el arreglo, retorna -1\n",
      "    return -1\n",
      "\n",
      "# Ejemplo de uso\n",
      "arr = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "objetivo = 5\n",
      "resultado = busqueda_binaria(arr, objetivo)\n",
      "\n",
      "if resultado != -1:\n",
      "    print(f\"El elemento {objetivo} se encuentra en el √≠ndice {resultado}.\")\n",
      "else:\n",
      "    print(f\"El elemento {objetivo} no se encuentra en el arreglo.\")\n",
      "```\n",
      "\n",
      "### Explicaci√≥n del c√≥digo:\n",
      "\n",
      "1. **Inicializaci√≥n de √≠ndices**: Se inicializan dos √≠ndices, `izquierda` y `derecha`, que representan los l√≠mites del arreglo donde se buscar√° el objetivo.\n",
      "\n",
      "2. **Bucle `while`**: El bucle contin√∫a mientras `izquierda` sea menor o igual a `derecha`. Esto asegura que el arreglo a√∫n tiene elementos que pueden contener el objetivo.\n",
      "\n",
      "3. **C√°lculo del √≠ndice medio**: Se calcula el √≠ndice medio del arreglo actual usando la f√≥rmula `(izquierda + derecha) // 2`.\n",
      "\n",
      "4. **Comparaci√≥n con el objetivo**:\n",
      "   - Si el elemento en el √≠ndice medio es igual al objetivo, se retorna el √≠ndice medio.\n",
      "   - Si el elemento medio es menor que el objetivo, se descarta la mitad izquierda del arreglo ajustando `izquierda` a `medio + 1`.\n",
      "   - Si el elemento medio es mayor que el objetivo, se descarta la mitad derecha del arreglo ajustando `derecha` a `medio - 1`.\n",
      "\n",
      "5. **Retorno de -1**: Si el bucle termina sin encontrar el objetivo, se retorna `-1` para indicar que el objetivo no est√° en el arreglo.\n",
      "\n",
      "### Ejemplo:\n",
      "Si buscas el n√∫mero `5` en el arreglo `[1, 2, 3, 4, 5, 6, 7, 8, 9]`, la funci√≥n retornar√° `4`, que es el √≠ndice donde se encuentra el n√∫mero `5`.\n",
      "\n",
      "Este algoritmo tiene una complejidad de tiempo de **O(log n)**, lo que lo hace muy eficiente para arreglos grandes y ordenados.\n",
      "================================================================================\n",
      "\n",
      "Experimento registrado en MLflow\n",
      "Registry: llm_deepseek_chat\n",
      "\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENTO: deepseek_question_answering\n",
      "Tarea: Respuesta a preguntas t√©cnicas\n",
      "================================================================================\n",
      "\n",
      " PROMPT:\n",
      "Explica qu√© es el aprendizaje por refuerzo en machine learning y proporciona un ejemplo pr√°ctico de su aplicaci√≥n.\n",
      "\n",
      "Generando respuesta...\n",
      "\n",
      "================================================================================\n",
      "RESPUESTA:\n",
      "================================================================================\n",
      "El **aprendizaje por refuerzo** (Reinforcement Learning, RL) es un tipo de aprendizaje autom√°tico en el que un agente aprende a tomar decisiones realizando acciones en un entorno para maximizar una recompensa acumulada a largo plazo. A diferencia del aprendizaje supervisado, donde el modelo se entrena con un conjunto de datos etiquetados, en el aprendizaje por refuerzo el agente aprende mediante la interacci√≥n con el entorno y recibe retroalimentaci√≥n en forma de recompensas o penalizaciones.\n",
      "\n",
      "### Componentes clave del aprendizaje por refuerzo:\n",
      "1. **Agente**: Entidad que toma decisiones y realiza acciones.\n",
      "2. **Entorno**: El mundo en el que el agente opera y con el que interact√∫a.\n",
      "3. **Acciones**: Las decisiones que el agente puede tomar.\n",
      "4. **Estado**: La situaci√≥n actual del entorno.\n",
      "5. **Recompensa**: La retroalimentaci√≥n que el agente recibe despu√©s de realizar una acci√≥n.\n",
      "6. **Pol√≠tica**: La estrategia que el agente sigue para decidir qu√© acci√≥n tomar en cada estado.\n",
      "7. **Funci√≥n de valor**: La estimaci√≥n de la recompensa acumulada futura que el agente puede obtener desde un estado determinado.\n",
      "\n",
      "### Proceso de aprendizaje:\n",
      "1. El agente observa el estado actual del entorno.\n",
      "2. Elige una acci√≥n basada en su pol√≠tica.\n",
      "3. Realiza la acci√≥n y recibe una recompensa.\n",
      "4. Transiciona a un nuevo estado.\n",
      "5. Aprende de la experiencia para mejorar su pol√≠tica y maximizar la recompensa futura.\n",
      "\n",
      "### Ejemplo pr√°ctico: Juego de ajedrez\n",
      "Un ejemplo cl√°sico de aprendizaje por refuerzo es entrenar un agente para jugar ajedrez.\n",
      "\n",
      "1. **Agente**: El programa que juega ajedrez.\n",
      "2. **Entorno**: El tablero de ajedrez y las reglas del juego.\n",
      "3. **Acciones**: Todos los movimientos legales disponibles en cada turno.\n",
      "4. **Estado**: La configuraci√≥n actual del tablero (posici√≥n de las piezas).\n",
      "5. **Recompensa**: Puede ser +1 por ganar, -1 por perder y 0 por empates o movimientos intermedios.\n",
      "6. **Pol√≠tica**: La estrategia que el agente utiliza para elegir movimientos (por ejemplo, minimizar el riesgo de perder).\n",
      "7. **Funci√≥n de valor**: La estimaci√≥n de la probabilidad de ganar desde una posici√≥n dada.\n",
      "\n",
      "El agente comienza jugando aleatoriamente, pero a medida que recibe recompensas (ganar, perder, empatar), aprende qu√© movimientos conducen a mejores resultados. Con el tiempo, desarrolla una pol√≠tica √≥ptima que maximiza sus posibilidades de ganar.\n",
      "\n",
      "### Aplicaciones pr√°cticas:\n",
      "- **Rob√≥tica**: Ense√±ar a robots a realizar tareas complejas, como caminar o manipular objetos.\n",
      "- **Juegos**: Entrenar agentes para juegos como Go, Dota 2 o StarCraft II.\n",
      "- **Recomendaciones**: Personalizar recomendaciones en plataformas como Netflix o Spotify.\n",
      "- **Control de sistemas**: Optimizar el funcionamiento de sistemas como la climatizaci√≥n o la gesti√≥n de energ√≠a.\n",
      "- **Autos aut√≥nomos**: Ense√±ar a veh√≠culos a tomar decisiones de conducci√≥n seguras.\n",
      "\n",
      "El aprendizaje por refuerzo es especialmente √∫til en problemas donde no hay un conjunto de datos etiquetado, pero s√≠ un criterio claro de √©xito o fracaso que gu√≠a el aprendizaje.\n",
      "================================================================================\n",
      "\n",
      "Experimento registrado en MLflow\n",
      "Registry: llm_deepseek_chat\n",
      "\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENTO: deepseek_summarization\n",
      "Tarea: Resumen y s√≠ntesis de informaci√≥n\n",
      "================================================================================\n",
      "\n",
      " PROMPT:\n",
      "Resume los principios fundamentales de la programaci√≥n orientada a objetos en 5 puntos clave.\n",
      "\n",
      "Generando respuesta...\n",
      "\n",
      "================================================================================\n",
      "RESPUESTA:\n",
      "================================================================================\n",
      "La programaci√≥n orientada a objetos (POO) se basa en cinco principios fundamentales:\n",
      "\n",
      "1. **Abstracci√≥n**: Representar las caracter√≠sticas esenciales de un objeto, ocultando los detalles innecesarios. Permite modelar problemas del mundo real de manera simplificada.\n",
      "\n",
      "2. **Encapsulaci√≥n**: Agrupar datos y m√©todos que operan sobre esos datos en una sola unidad (clase), controlando el acceso a ellos. Protege la integridad de los datos y oculta la implementaci√≥n interna.\n",
      "\n",
      "3. **Herencia**: Permitir que una clase (subclase) derive caracter√≠sticas y comportamientos de otra clase (superclase). Facilita la reutilizaci√≥n de c√≥digo y la creaci√≥n de jerarqu√≠as de clases.\n",
      "\n",
      "4. **Polimorfismo**: Capacidad de que un mismo m√©todo o funci√≥n opere de manera diferente seg√∫n el objeto que lo invoque. Permite tratar objetos de diferentes clases de manera uniforme.\n",
      "\n",
      "5. **Modularidad**: Dividir un programa en componentes independientes y reutilizables (clases o m√≥dulos). Facilita el mantenimiento, la escalabilidad y la organizaci√≥n del c√≥digo. \n",
      "\n",
      "Estos principios permiten crear software estructurado, mantenible y escalable.\n",
      "================================================================================\n",
      "\n",
      "Experimento registrado en MLflow\n",
      "Registry: llm_deepseek_chat\n",
      "\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENTO: deepseek_translation\n",
      "Tarea: Traducci√≥n de texto t√©cnico\n",
      "================================================================================\n",
      "\n",
      " PROMPT:\n",
      "Traduce el siguiente texto al ingl√©s de manera natural: 'El machine learning ha revolucionado la forma en que procesamos y analizamos grandes vol√∫menes de datos en tiempo real.'\n",
      "\n",
      "Generando respuesta...\n",
      "\n",
      "================================================================================\n",
      "RESPUESTA:\n",
      "================================================================================\n",
      "\"Machine learning has revolutionized the way we process and analyze large volumes of data in real time.\"\n",
      "================================================================================\n",
      "\n",
      "Experimento registrado en MLflow\n",
      "Registry: llm_deepseek_chat\n",
      "\n"
     ]
    }
   ],
   "execution_count": 39
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
